from dataclasses import dataclass, field
from typing import List, Optional
from trl.core import flatten_dict


@dataclass
class ICLConfig:
    """
    Arguments which define the model and tokenizer to load.
    """
    output_dir: str = field(
        metadata={
            "help": "The output directory where the model predictions and checkpoints will be written."
        },
    )
    seed: int = field(default=88, metadata={"help": "Random seed that will be set at the beginning of training."})
    last_generation_dir: str = field(
        default=None,
        metadata={
            "help": "The directory of last generation results, for iterative refinement"
        },
    )
    last_generation_as: str = field(
        default="prediction:hypothesis",
        metadata={
            "help": "incorporate the data of last generation to the currect dataset. Here you should input the mapping method of key. e.g., a1:a2,b1:b2"
        },
    )
    key_text_dir: str = field(
        default=None,
        metadata={
            "help": "The directory for generated query key. Should be result directory under the same setting of argument `icl_datasets`. The key will extracted from the `response` of each data item."
        },
    )
    key_preprocess: str = field(
        default=None,
        metadata={
            "help": "The preprocess for key text after loading keys from key_text_dir (and example selection mode is not `text`). Please refer to icl_retrieval.py. "
        },
    )
    retrieval_mode: str = field(
        default=None,
        metadata={"help": "In context example retrieval mode in icl_retrieval.py"}
    )
    examples_dir: str = field(
        default=None,
        metadata={
            "help": "The directory for retrieved data examples. Should be result directory under the same setting of argument `icl_datasets`."
        },
    )
    max_new_tokens: int = field(
        default=512,
        metadata={
            "help": ("Max new tokens generated by model.")
        }
    )
    in_domain_example_num: int = field(
        default=0,
        metadata={
            "help": ("The number of in-domain examples selected. In domain datasets are set in the `datasets` argument.")
        }
    )
    cross_domain_example_num: int = field(
        default=0,
        metadata={
            "help": ("The number of all-domain examples selected. The datasets are set in the `icl_datasets` argument or recognized automatically by the `datasets` argument.")
        }
    )
    in_domain_example_mode: str = field(
        default='default',
        metadata={
            "help": ("Example selection mode of in-domain datasets.")
        }
    )
    cross_domain_example_mode: str = field(
        default='default',
        metadata={
            "help": ("Example selection mode of cross-domain datasets.")
        }
    )
    in_domain_filter: str = field(
        default='',
        metadata={
            "help": ("Example filter of in-domain datasets.")
        }
    )
    cross_domain_filter: str = field(
        default='',
        metadata={
            "help": ("Example filter of cross-domain datasets.")
        }
    )
    do_sample: bool = field(
        default=False,
        metadata={
            "help": ("Whether to use sampling or greedy decoding during generation.")
        }
    )
    top_k: Optional[int] = field(
        default=None,
        metadata={
            "help": ("Number of highest probability vocabulary tokens to keep for top-k filtering.")
        }
    )
    top_p: float = field(
        default=0.,
        metadata={
            "help": ("Magnitude to filter tokens by probability (0.0 means no filtering).")
        }
    )
    temperature: float = field(
        default=0.,
        metadata={
            "help": ("Temperature to adjust the probability distribution of generated tokens.")
        }
    )
    postprocess: str = field(
        default='no',
        metadata={
            "help": ("Postprocess type for the final generated text.")
        }
    )
    dialogue_form: bool = field(
        default=False,
        metadata={
            "help": ("Conduct dialogue template to continue the text generation in dialogue-supported models.")
        } 
    )
    database: bool = field(
        default=False,
        metadata={
            "help": ("Whether to use database for icl example selection.")
        }
    )
    database_dir: str = field(
        default='.cache',
        metadata={
            "help": ("Store database under the directory.")
        }
    )
    database_parameters: str = field(
        default='{"num_limit":25000,"min_len":10,"max_len":60}',
        metadata={
            "help": ("A dict of database config parameters. e.g. {'num_limit':40000,'min_len':8,'max_len':100}")
        }
    )
    medium_result_dir: str = field(
        default='.cache/llm',
        metadata={
            "help": ("Store medium result while preparing ICL examples under the directory.")
        }
    )
    medium_result_postprocess: str = field(
        default=None,
        metadata={
            "help": ("The postprocess for medium text before apply to database, after generation of medium result. Please refer to database.py. ")
        }
    )
    assist_model: str = field(
        default="",
        metadata={
            "help": ("Assist chat model in the database construction.")
        }
    )
    assist_prompt: str = field(
        default="",
        metadata={
            "help": ("Prompt used by assist chat model.")
        }
    )
    embedding_model: str = field(
        default="",
        metadata={
            "help": ("Embedding model in the database construction.")
        }
    )
    embedding_batch_size: str = field(
        default=128,
        metadata={
            "help": ("Embedding model compute batch size.")
        }
    )

    def to_dict(self):
        output_dict = {}
        for key, value in self.__dict__.items():
            output_dict[key] = value
        return flatten_dict(output_dict)
    
    def __post_init__(self):
        pass
